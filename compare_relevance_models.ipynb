{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a320d553",
   "metadata": {},
   "source": [
    "# Relevance model comparison (imbalanced binary classification)\n",
    "\n",
    "This notebook compares **three relevance models** on the same split and metrics:\n",
    "\n",
    "1. **TF-IDF + Logistic Regression** (baseline, very strong)\n",
    "2. **TF-IDF + LinearSVC** (often better recall)\n",
    "3. **TF-IDF + LightGBM** (best non-linear option)\n",
    "\n",
    "### Why these models\n",
    "- Data is **highly imbalanced** (relevant=1 is rare)\n",
    "- Task is **binary filtering**\n",
    "- Metrics focus on **PR-AUC** and **recall at high precision**\n",
    "\n",
    "### What this notebook does\n",
    "- Loads a CSV with columns: `text` and `relevant` (or `relevance`)\n",
    "- Creates a **single stratified train/val split**\n",
    "- Runs **sensible hyperparameter search** per model (not insane grids)\n",
    "- Evaluates with:\n",
    "  - PR-AUC\n",
    "  - Recall @ Precision ≥ 0.90\n",
    "  - F1 (positive class)\n",
    "- Prints a **final comparison table**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998d57c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Imports & config\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.30\n",
    "TARGET_PRECISION = 0.90\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12f56a9",
   "metadata": {},
   "source": [
    "## 1) Load data\n",
    "\n",
    "Edit the path below. Required columns:\n",
    "- `text`\n",
    "- `relevant` **or** `relevance`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48db5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: update path\n",
    "DATASET_CSV = Path(\"dataset.csv\")\n",
    "\n",
    "df = pd.read_csv(DATASET_CSV)\n",
    "\n",
    "# detect relevance column\n",
    "if \"relevant\" in df.columns:\n",
    "    REL_COL = \"relevant\"\n",
    "elif \"relevance\" in df.columns:\n",
    "    REL_COL = \"relevance\"\n",
    "else:\n",
    "    raise ValueError(\"Expected column `relevant` or `relevance`\")\n",
    "\n",
    "TEXT_COL = \"text\"\n",
    "\n",
    "df[REL_COL] = df[REL_COL].fillna(0).astype(int)\n",
    "df = df.dropna(subset=[TEXT_COL]).copy()\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Class distribution:\")\n",
    "print(df[REL_COL].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518b2de2",
   "metadata": {},
   "source": [
    "## 2) Train / validation split (stratified)\n",
    "\n",
    "We stratify by relevance to keep positives in both splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1267c40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df[TEXT_COL].astype(str),\n",
    "    df[REL_COL].values,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=df[REL_COL] if df[REL_COL].nunique() > 1 else None,\n",
    ")\n",
    "\n",
    "print(\"Train:\", len(X_train), \"Val:\", len(X_val))\n",
    "print(\"Train positives:\", y_train.sum(), \"Val positives:\", y_val.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea55dc2",
   "metadata": {},
   "source": [
    "## 3) Helper metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562815c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_precision(y_true, y_score, target_precision=0.90):\n",
    "    p, r, _ = precision_recall_curve(y_true, y_score)\n",
    "    p, r = p[:-1], r[:-1]\n",
    "    ok = np.where(p >= target_precision)[0]\n",
    "    if len(ok) == 0:\n",
    "        return 0.0\n",
    "    return float(np.max(r[ok]))\n",
    "\n",
    "\n",
    "def evaluate_binary(y_true, y_score, threshold=0.5):\n",
    "    pr_auc = average_precision_score(y_true, y_score)\n",
    "    y_pred = (y_score >= threshold).astype(int)\n",
    "    f1 = f1_score(y_true, y_pred, pos_label=1)\n",
    "    r_at_p = recall_at_precision(y_true, y_score, TARGET_PRECISION)\n",
    "    return {\n",
    "        \"pr_auc\": pr_auc,\n",
    "        \"f1@0.5\": f1,\n",
    "        f\"recall@P>={TARGET_PRECISION}\": r_at_p,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f26b66",
   "metadata": {},
   "source": [
    "## 4) Model 1 — TF-IDF + Logistic Regression\n",
    "\n",
    "Why this search:\n",
    "- `ngram_range`: (1,2) vs (1,3) → key trade-off\n",
    "- `C`: controls regularization (most important)\n",
    "- `class_weight=balanced`: **must** for imbalance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae58357",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        min_df=2,\n",
    "        max_df=0.95,\n",
    "        strip_accents=None,\n",
    "    )),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        max_iter=3000,\n",
    "        class_weight=\"balanced\",\n",
    "        solver=\"liblinear\",\n",
    "    )),\n",
    "])\n",
    "\n",
    "logreg_params = {\n",
    "    \"tfidf__ngram_range\": [(1,2), (1,3)],\n",
    "    \"clf__C\": [0.1, 0.5, 1.0, 2.0],\n",
    "}\n",
    "\n",
    "logreg_search = RandomizedSearchCV(\n",
    "    logreg_pipe,\n",
    "    logreg_params,\n",
    "    n_iter=6,\n",
    "    scoring=\"average_precision\",\n",
    "    cv=3,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "logreg_search.fit(X_train, y_train)\n",
    "logreg_best = logreg_search.best_estimator_\n",
    "\n",
    "logreg_scores = evaluate_binary(\n",
    "    y_val,\n",
    "    logreg_best.predict_proba(X_val)[:,1],\n",
    ")\n",
    "\n",
    "print(\"Best params:\", logreg_search.best_params_)\n",
    "print(\"Scores:\", logreg_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435d37b5",
   "metadata": {},
   "source": [
    "## 5) Model 2 — TF-IDF + LinearSVC (calibrated)\n",
    "\n",
    "Why:\n",
    "- SVC often gives **better recall**\n",
    "- Needs **calibration** to produce probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ddcb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        min_df=2,\n",
    "        max_df=0.95,\n",
    "        strip_accents=None,\n",
    "    )),\n",
    "    (\"clf\", LinearSVC(\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=5000,\n",
    "    )),\n",
    "])\n",
    "\n",
    "svc_params = {\n",
    "    \"tfidf__ngram_range\": [(1,2), (1,3)],\n",
    "    \"clf__C\": [0.1, 0.5, 1.0, 2.0],\n",
    "}\n",
    "\n",
    "svc_search = RandomizedSearchCV(\n",
    "    svc_pipe,\n",
    "    svc_params,\n",
    "    n_iter=6,\n",
    "    scoring=\"average_precision\",\n",
    "    cv=3,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "svc_search.fit(X_train, y_train)\n",
    "svc_best = svc_search.best_estimator_\n",
    "\n",
    "# calibrate to get probabilities\n",
    "svc_cal = CalibratedClassifierCV(svc_best, method=\"sigmoid\", cv=3)\n",
    "svc_cal.fit(X_train, y_train)\n",
    "\n",
    "svc_scores = evaluate_binary(\n",
    "    y_val,\n",
    "    svc_cal.predict_proba(X_val)[:,1],\n",
    ")\n",
    "\n",
    "print(\"Best params:\", svc_search.best_params_)\n",
    "print(\"Scores:\", svc_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b606a74d",
   "metadata": {},
   "source": [
    "## 6) Model 3 — TF-IDF + LightGBM\n",
    "\n",
    "Why:\n",
    "- Non-linear interactions of n-grams\n",
    "- Often improves recall on tricky cases\n",
    "\n",
    "Notes:\n",
    "- We **limit features** to avoid overfitting\n",
    "- `scale_pos_weight` handles imbalance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb93eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize separately for LightGBM (sparse matrix)\n",
    "tfidf_lgbm = TfidfVectorizer(\n",
    "    min_df=3,\n",
    "    max_df=0.9,\n",
    "    ngram_range=(1,2),\n",
    "    max_features=200_000,\n",
    ")\n",
    "\n",
    "Xtr_lgbm = tfidf_lgbm.fit_transform(X_train)\n",
    "Xva_lgbm = tfidf_lgbm.transform(X_val)\n",
    "\n",
    "pos_weight = (len(y_train) - y_train.sum()) / max(1, y_train.sum())\n",
    "\n",
    "lgbm = lgb.LGBMClassifier(\n",
    "    objective=\"binary\",\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=63,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=pos_weight,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "lgbm.fit(Xtr_lgbm, y_train)\n",
    "\n",
    "lgbm_scores = evaluate_binary(\n",
    "    y_val,\n",
    "    lgbm.predict_proba(Xva_lgbm)[:,1],\n",
    ")\n",
    "\n",
    "print(\"Scores:\", lgbm_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9df1fbb",
   "metadata": {},
   "source": [
    "## 7) Final comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b584566",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame.from_dict({\n",
    "    \"TF-IDF + LogReg\": logreg_scores,\n",
    "    \"TF-IDF + LinearSVC\": svc_scores,\n",
    "    \"TF-IDF + LightGBM\": lgbm_scores,\n",
    "}, orient=\"index\")\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d519b62",
   "metadata": {},
   "source": [
    "## How to interpret results\n",
    "\n",
    "- **PR-AUC** → main metric (higher is better)\n",
    "- **Recall@P≥0.90** → how many relevant sentences you keep at high precision\n",
    "- If two models are close:\n",
    "  - prefer **simpler (LogReg / SVC)**\n",
    "  - LightGBM only if it gives a **clear** win\n",
    "\n",
    "Typical outcome on such data:\n",
    "- LogReg = very strong baseline\n",
    "- LinearSVC = slightly better recall\n",
    "- LightGBM = best recall, but risk of overfitting\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
