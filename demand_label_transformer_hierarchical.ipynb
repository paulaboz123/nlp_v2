{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0380a164",
   "metadata": {},
   "source": [
    "# Demand label classification — Transformer multiclass (hierarchical-aware, weighted)\n",
    "\n",
    "This notebook presents **another strong solution** for your problem, fully transformer-based,\n",
    "*without* description pairing and *without* kNN:\n",
    "\n",
    "## When to use this solution\n",
    "- You want **one transformer model** (simpler infra than cross-encoder)\n",
    "- You have **hierarchical labels** (group → fine)\n",
    "- You want to handle:\n",
    "  - 200+ labels\n",
    "  - severe imbalance\n",
    "  - very similar labels\n",
    "\n",
    "## Core idea\n",
    "Instead of flat softmax over 200+ labels, we use:\n",
    "\n",
    "### 1) Shared encoder (Transformer)\n",
    "### 2) Group-aware heads\n",
    "- One classification head **per group**\n",
    "- Loss computed **only for the true group**\n",
    "- Strongly reduces confusion between unrelated labels\n",
    "\n",
    "This is often called:\n",
    "- *hierarchical softmax*\n",
    "- *conditional computation*\n",
    "- *grouped classification heads*\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline\n",
    "1. Filter `relevant==1`\n",
    "2. Train **group classifier** (transformer)\n",
    "3. Train **fine-label transformer** with group-aware heads\n",
    "\n",
    "This notebook implements **step 3** assuming group is known during training.\n",
    "At inference, you first predict group, then use the corresponding head.\n",
    "\n",
    "---\n",
    "\n",
    "## Pros vs cross-encoder\n",
    "+ Faster inference (one forward pass)\n",
    "+ Easier serving\n",
    "+ Better when you have more data per label\n",
    "\n",
    "## Cons\n",
    "- Slightly worse on ultra-fine distinctions than cross-encoder\n",
    "- Requires careful loss handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f26bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CONFIG (EDIT THESE)\n",
    "# =========================\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_CSV = Path(\"dataset.csv\")\n",
    "\n",
    "TEXT_COL = \"text\"\n",
    "DEMAND_COL = \"demand_id\"\n",
    "GROUP_COL = \"group_id\"\n",
    "REL_COL_CANDIDATES = [\"relevant\", \"relevance\"]\n",
    "\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "MAX_LEN = 256\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 8\n",
    "LR = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "SEED = 42\n",
    "TEST_SIZE = 0.30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efa22e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bbbf01",
   "metadata": {},
   "source": [
    "## 1) Load data and keep only relevant==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60195c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET_CSV)\n",
    "\n",
    "# detect relevance column\n",
    "rel_col = None\n",
    "for c in REL_COL_CANDIDATES:\n",
    "    if c in df.columns:\n",
    "        rel_col = c\n",
    "        break\n",
    "if rel_col is None:\n",
    "    raise ValueError(\"Missing relevance column\")\n",
    "\n",
    "df = df[df[rel_col].fillna(0).astype(int) == 1].copy()\n",
    "\n",
    "for c in [TEXT_COL, DEMAND_COL, GROUP_COL]:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f\"Missing column: {c}\")\n",
    "\n",
    "df[DEMAND_COL] = df[DEMAND_COL].astype(str)\n",
    "df[GROUP_COL] = df[GROUP_COL].astype(str)\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Groups:\", df[GROUP_COL].nunique())\n",
    "print(\"Labels:\", df[DEMAND_COL].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cfc438",
   "metadata": {},
   "source": [
    "## 2) Encode groups and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b005ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_encoder = LabelEncoder()\n",
    "df[\"group_idx\"] = group_encoder.fit_transform(df[GROUP_COL])\n",
    "\n",
    "label_encoders = {}\n",
    "df[\"label_idx\"] = -1\n",
    "\n",
    "for g, sub in df.groupby(\"group_idx\"):\n",
    "    le = LabelEncoder()\n",
    "    idx = sub.index\n",
    "    df.loc[idx, \"label_idx\"] = le.fit_transform(sub[DEMAND_COL])\n",
    "    label_encoders[g] = le\n",
    "\n",
    "assert (df[\"label_idx\"] >= 0).all()\n",
    "\n",
    "print(\"Example group sizes:\")\n",
    "print(df.groupby(\"group_idx\")[\"label_idx\"].nunique().head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf90f60",
   "metadata": {},
   "source": [
    "## 3) Train/val split (stratified by group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a467fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=SEED,\n",
    "    stratify=df[\"group_idx\"] if df[\"group_idx\"].nunique() > 1 else None,\n",
    ")\n",
    "\n",
    "print(\"Train:\", train_df.shape, \"Val:\", val_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6720ef",
   "metadata": {},
   "source": [
    "## 4) Dataset + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845430e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[TEXT_COL],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df[[TEXT_COL, \"group_idx\", \"label_idx\"]]).map(tokenize, batched=True)\n",
    "val_ds = Dataset.from_pandas(val_df[[TEXT_COL, \"group_idx\", \"label_idx\"]]).map(tokenize, batched=True)\n",
    "\n",
    "cols = [\"input_ids\", \"attention_mask\", \"group_idx\", \"label_idx\"]\n",
    "train_ds.set_format(type=\"torch\", columns=cols)\n",
    "val_ds.set_format(type=\"torch\", columns=cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24fdb97",
   "metadata": {},
   "source": [
    "## 5) Model: shared encoder + group-specific heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f86763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalClassifier(nn.Module):\n",
    "    def __init__(self, base_model_name, num_labels_per_group):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(base_model_name)\n",
    "        hidden = self.encoder.config.hidden_size\n",
    "\n",
    "        self.heads = nn.ModuleDict({\n",
    "            str(g): nn.Linear(hidden, n_labels)\n",
    "            for g, n_labels in num_labels_per_group.items()\n",
    "        })\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, group_idx, labels=None):\n",
    "        enc = self.encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0]\n",
    "        logits = []\n",
    "\n",
    "        loss = None\n",
    "        for i in range(enc.size(0)):\n",
    "            g = str(group_idx[i].item())\n",
    "            logit = self.heads[g](enc[i])\n",
    "            logits.append(logit)\n",
    "\n",
    "        logits = torch.nn.utils.rnn.pad_sequence(logits, batch_first=True)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "\n",
    "num_labels_per_group = train_df.groupby(\"group_idx\")[\"label_idx\"].nunique().to_dict()\n",
    "model = HierarchicalClassifier(MODEL_NAME, num_labels_per_group).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d91f17",
   "metadata": {},
   "source": [
    "## 6) Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2aa793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"label_idx\")\n",
    "        group_idx = inputs.pop(\"group_idx\")\n",
    "        outputs = model(**inputs, group_idx=group_idx, labels=labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"hier_transformer_out\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a953f495",
   "metadata": {},
   "source": [
    "## 7) Evaluation (per-group reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a18314",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for row in val_ds:\n",
    "        inputs = {\n",
    "            \"input_ids\": row[\"input_ids\"].unsqueeze(0).to(device),\n",
    "            \"attention_mask\": row[\"attention_mask\"].unsqueeze(0).to(device),\n",
    "        }\n",
    "        g = row[\"group_idx\"].item()\n",
    "        logits = model(**inputs, group_idx=torch.tensor([g]).to(device))[\"logits\"]\n",
    "        pred = logits.argmax(dim=-1).item()\n",
    "        y_true.append(row[\"label_idx\"].item())\n",
    "        y_pred.append(pred)\n",
    "\n",
    "print(classification_report(y_true, y_pred, zero_division=0))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
