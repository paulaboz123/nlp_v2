{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58447df4",
   "metadata": {},
   "source": [
    "# Full end-to-end NLP pipeline (one notebook)\n",
    "\n",
    "**Steps**\n",
    "1. Merge train+test → drop excluded `demand_id` → resplit 70/30 (single-sample labels stay in train)\n",
    "2. Relevance model comparison: TF‑IDF + (LogReg, LinearSVC, optional LightGBM) + PR curves + best selection\n",
    "3. Label classification (relevant==1):\n",
    "   - Group classifier (TF‑IDF + LinearSVC calibrated)\n",
    "   - Model A: Description-aware Cross‑Encoder + Top‑N groups + Hard Negatives\n",
    "   - Model B: Hierarchical Transformer (shared encoder + per-group head)\n",
    "   - Compare + pick best + save artifacts\n",
    "\n",
    "> Notes: This notebook is written to be **runnable** and **debuggable** (prints progress, saves intermediate files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd77799",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# PARAMETERS (Papermill / AML)\n",
    "# =========================\n",
    "# If you run this notebook locally, you can edit these.\n",
    "# If you run it on Azure ML, the command job uses Papermill to inject values.\n",
    "\n",
    "TRAIN_CSV  = \"train.csv\"\n",
    "TEST_CSV   = \"test.csv\"\n",
    "LABELS_CSV = \"labels.csv\"\n",
    "\n",
    "# Core columns\n",
    "TEXT_COL   = \"text\"\n",
    "DEMAND_COL = \"demand_id\"\n",
    "GROUP_COL  = \"group_id\"\n",
    "REL_COL_CANDIDATES = [\"relevant\", \"relevance\"]  # 0/1\n",
    "\n",
    "# Filtering\n",
    "EXCLUDE_COL = \"exclude\"  # labels.csv: 1 => drop\n",
    "\n",
    "# Splits / eval\n",
    "SEED = 42\n",
    "TEST_SIZE_RESPLIT = 0.30      # 70/30\n",
    "TEST_SIZE_RELEVANCE = 0.30    # relevance model eval split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ba265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Runtime paths / outputs\n",
    "# -------------------------\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "TRAIN_CSV  = Path(TRAIN_CSV)\n",
    "TEST_CSV   = Path(TEST_CSV)\n",
    "LABELS_CSV = Path(LABELS_CSV)\n",
    "\n",
    "# In Azure ML command jobs, write artifacts to ./outputs (gets uploaded automatically).\n",
    "OUTPUT_DIR = Path(os.getenv(\"OUTPUT_DIR\", \"outputs\"))\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"TRAIN_CSV:\", TRAIN_CSV.resolve())\n",
    "print(\"TEST_CSV :\", TEST_CSV.resolve())\n",
    "print(\"LABELS_CSV:\", LABELS_CSV.resolve())\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59ab0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os, json, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, auc\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LGBM = True\n",
    "except Exception:\n",
    "    HAS_LGBM = False\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModel,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE, \"| LightGBM:\", HAS_LGBM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e1fc41",
   "metadata": {},
   "source": [
    "## 1) Step 00 — Filter excluded labels & resplit 70/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a416fc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inputs\n",
    "train_raw = pd.read_csv(TRAIN_CSV)\n",
    "test_raw  = pd.read_csv(TEST_CSV)\n",
    "labels_df = pd.read_csv(LABELS_CSV)\n",
    "\n",
    "# detect relevance column\n",
    "rel_col = None\n",
    "for c in REL_COL_CANDIDATES:\n",
    "    if c in train_raw.columns:\n",
    "        rel_col = c\n",
    "        break\n",
    "if rel_col is None:\n",
    "    raise ValueError(f\"Could not find relevance column. Tried: {REL_COL_CANDIDATES}. Found: {list(train_raw.columns)}\")\n",
    "\n",
    "# Normalize dtypes\n",
    "for df_ in (train_raw, test_raw):\n",
    "    df_[DEMAND_COL] = df_[DEMAND_COL].astype(str)\n",
    "    if GROUP_COL in df_.columns:\n",
    "        df_[GROUP_COL] = df_[GROUP_COL].astype(str)\n",
    "\n",
    "labels_df[DEMAND_COL] = labels_df[DEMAND_COL].astype(str)\n",
    "\n",
    "# Merge\n",
    "merged = pd.concat([train_raw, test_raw], ignore_index=True)\n",
    "print(\"Merged rows:\", len(merged))\n",
    "\n",
    "# Filter excluded demand_ids\n",
    "excluded = set(labels_df.loc[labels_df[EXCLUDE_COL] == 1, DEMAND_COL].astype(str))\n",
    "before = len(merged)\n",
    "merged = merged[~merged[DEMAND_COL].isin(excluded)].copy()\n",
    "print(\"Excluded labels:\", len(excluded), \"| Rows removed:\", before - len(merged))\n",
    "print(\"Rows after filter:\", len(merged), \"| Labels:\", merged[DEMAND_COL].nunique())\n",
    "\n",
    "# Guard: labels with only 1 row must stay in train\n",
    "counts = merged[DEMAND_COL].value_counts()\n",
    "single_labels = set(counts[counts == 1].index)\n",
    "\n",
    "df_single = merged[merged[DEMAND_COL].isin(single_labels)].copy()\n",
    "df_multi  = merged[~merged[DEMAND_COL].isin(single_labels)].copy()\n",
    "\n",
    "# If only one label overall, keep all in train\n",
    "if df_multi[DEMAND_COL].nunique() <= 1:\n",
    "    train_clean = merged.copy()\n",
    "    test_clean = merged.iloc[0:0].copy()\n",
    "    print(\"Only one label overall → keeping all rows in train, empty test.\")\n",
    "else:\n",
    "    train_multi, test_multi = train_test_split(\n",
    "        df_multi,\n",
    "        test_size=TEST_SIZE_RESPLIT,\n",
    "        random_state=SEED,\n",
    "        stratify=df_multi[DEMAND_COL]\n",
    "    )\n",
    "    train_clean = pd.concat([train_multi, df_single], ignore_index=True)\n",
    "    test_clean  = test_multi.copy()\n",
    "\n",
    "print(\"Train_clean:\", len(train_clean), \"rows | labels:\", train_clean[DEMAND_COL].nunique())\n",
    "print(\"Test_clean :\", len(test_clean),  \"rows | labels:\", test_clean[DEMAND_COL].nunique())\n",
    "\n",
    "# Save\n",
    "train_clean_path = ARTIFACTS_DIR / \"train_clean.csv\"\n",
    "test_clean_path  = ARTIFACTS_DIR / \"test_clean.csv\"\n",
    "train_clean.to_csv(train_clean_path, index=False)\n",
    "test_clean.to_csv(test_clean_path, index=False)\n",
    "print(\"Saved:\", train_clean_path, test_clean_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fb5e56",
   "metadata": {},
   "source": [
    "## 2) Relevance model — compare TF‑IDF + (LogReg, LinearSVC, LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eda670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare relevance dataset (from train_clean only)\n",
    "df_rel = train_clean.copy()\n",
    "df_rel[rel_col] = df_rel[rel_col].fillna(0).astype(int)\n",
    "\n",
    "X = df_rel[TEXT_COL].astype(str)\n",
    "y = df_rel[rel_col].astype(int)\n",
    "\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE_RELEVANCE, random_state=SEED, stratify=y if y.nunique() > 1 else None\n",
    ")\n",
    "\n",
    "def recall_at_precision(y_true, scores, precision_target=0.90):\n",
    "    p, r, thr = precision_recall_curve(y_true, scores)\n",
    "    # p,r length = len(thr)+1, threshold aligns with p[1:], r[1:]\n",
    "    best = 0.0\n",
    "    best_thr = None\n",
    "    for i in range(1, len(p)):\n",
    "        if p[i] >= precision_target:\n",
    "            if r[i] > best:\n",
    "                best = r[i]\n",
    "                best_thr = thr[i-1]\n",
    "    return best, best_thr\n",
    "\n",
    "def fit_eval_model(name, estimator, use_proba=True):\n",
    "    pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(1,3), min_df=2, max_df=0.95)),\n",
    "        (\"clf\", estimator),\n",
    "    ])\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "\n",
    "    if hasattr(estimator, \"predict_proba\") and use_proba:\n",
    "        scores = pipe.predict_proba(X_va)[:, 1]\n",
    "    elif hasattr(estimator, \"decision_function\"):\n",
    "        scores = pipe.decision_function(X_va)\n",
    "    else:\n",
    "        # fallback\n",
    "        scores = pipe.predict(X_va).astype(float)\n",
    "\n",
    "    pr_auc = average_precision_score(y_va, scores)\n",
    "    rec, thr = recall_at_precision(y_va, scores, precision_target=PRECISION_TARGET)\n",
    "    return pipe, {\"model\": name, \"pr_auc\": pr_auc, f\"recall@p>={PRECISION_TARGET}\": rec, \"threshold\": thr}\n",
    "\n",
    "models = []\n",
    "artifacts = {}\n",
    "\n",
    "# LogReg\n",
    "logreg = LogisticRegression(max_iter=5000, class_weight=\"balanced\", n_jobs=None)\n",
    "pipe_lr, m_lr = fit_eval_model(\"tfidf_logreg\", logreg, use_proba=True)\n",
    "models.append(m_lr); artifacts[m_lr[\"model\"]] = pipe_lr\n",
    "\n",
    "# LinearSVC (calibrated => proba)\n",
    "svc = LinearSVC(class_weight=\"balanced\", max_iter=8000)\n",
    "pipe_svc = Pipeline([(\"tfidf\", TfidfVectorizer(ngram_range=(1,3), min_df=2, max_df=0.95)), (\"clf\", svc)])\n",
    "pipe_svc.fit(X_tr, y_tr)\n",
    "cal_svc = CalibratedClassifierCV(pipe_svc, method=\"sigmoid\", cv=3)\n",
    "cal_svc.fit(X_tr, y_tr)\n",
    "scores = cal_svc.predict_proba(X_va)[:,1]\n",
    "m_svc = {\"model\":\"tfidf_linearsvc_cal\", \"pr_auc\": average_precision_score(y_va, scores)}\n",
    "rec, thr = recall_at_precision(y_va, scores, precision_target=PRECISION_TARGET)\n",
    "m_svc[f\"recall@p>={PRECISION_TARGET}\"] = rec\n",
    "m_svc[\"threshold\"] = thr\n",
    "models.append(m_svc); artifacts[m_svc[\"model\"]] = cal_svc\n",
    "\n",
    "# LightGBM (optional)\n",
    "if HAS_LGBM:\n",
    "    lgbm = lgb.LGBMClassifier(\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=64,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_lambda=1.0,\n",
    "        class_weight=None,  # we'll use scale_pos_weight\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    # compute scale_pos_weight\n",
    "    pos = int((y_tr == 1).sum()); neg = int((y_tr == 0).sum())\n",
    "    lgbm.set_params(scale_pos_weight=(neg/pos) if pos > 0 else 1.0)\n",
    "    pipe_lgbm, m_lgbm = fit_eval_model(\"tfidf_lgbm\", lgbm, use_proba=True)\n",
    "    models.append(m_lgbm); artifacts[m_lgbm[\"model\"]] = pipe_lgbm\n",
    "else:\n",
    "    print(\"LightGBM not installed. Skipping tfidf_lgbm. (Notebook still fully works.)\")\n",
    "\n",
    "results_rel = pd.DataFrame(models).sort_values([\"pr_auc\", f\"recall@p>={PRECISION_TARGET}\"], ascending=False)\n",
    "results_rel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0203e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PR curves for compared relevance models\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "def pr_curve_from_model(model_obj):\n",
    "    if hasattr(model_obj, \"predict_proba\"):\n",
    "        s = model_obj.predict_proba(X_va)[:,1]\n",
    "    else:\n",
    "        s = model_obj.decision_function(X_va)\n",
    "    p, r, _ = precision_recall_curve(y_va, s)\n",
    "    ap = average_precision_score(y_va, s)\n",
    "    return r, p, ap\n",
    "\n",
    "for name, mdl in artifacts.items():\n",
    "    r, p, ap = pr_curve_from_model(mdl)\n",
    "    plt.plot(r, p, label=f\"{name} (AP={ap:.3f})\")\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Relevance: Precision-Recall\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a6cd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best relevance model and save\n",
    "best_rel = results_rel.iloc[0][\"model\"]\n",
    "best_rel_model = artifacts[best_rel]\n",
    "best_rel_threshold = float(results_rel.iloc[0][\"threshold\"]) if results_rel.iloc[0][\"threshold\"] is not None else 0.0\n",
    "\n",
    "joblib.dump(\n",
    "    {\"model\": best_rel_model, \"threshold\": best_rel_threshold, \"rel_col\": rel_col},\n",
    "    ARTIFACTS_DIR / \"relevance_model.joblib\"\n",
    ")\n",
    "print(\"Best relevance model:\", best_rel, \"| threshold:\", best_rel_threshold)\n",
    "print(\"Saved:\", ARTIFACTS_DIR / \"relevance_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dccbc88",
   "metadata": {},
   "source": [
    "## 3) Label classification (relevant==1) — train group model + two label models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c621334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use relevant-only rows for label classification\n",
    "df_labels = train_clean.copy()\n",
    "df_labels[rel_col] = df_labels[rel_col].fillna(0).astype(int)\n",
    "df_labels = df_labels[df_labels[rel_col] == 1].copy()\n",
    "\n",
    "# Merge label descriptions (needed for cross-encoder)\n",
    "labels_map = labels_df[[DEMAND_COL]].copy()\n",
    "if \"description\" in labels_df.columns:\n",
    "    labels_map[\"description\"] = labels_df[\"description\"]\n",
    "else:\n",
    "    # If labels.csv does not include descriptions, you must add them\n",
    "    raise ValueError(\"labels.csv must contain 'description' column for cross-encoder solution.\")\n",
    "labels_map[DEMAND_COL] = labels_map[DEMAND_COL].astype(str)\n",
    "\n",
    "df_labels[DEMAND_COL] = df_labels[DEMAND_COL].astype(str)\n",
    "df_labels[GROUP_COL]  = df_labels[GROUP_COL].astype(str)\n",
    "\n",
    "df_labels = df_labels.merge(labels_map[[DEMAND_COL, \"description\"]], on=DEMAND_COL, how=\"left\")\n",
    "df_labels = df_labels.dropna(subset=[TEXT_COL, DEMAND_COL, GROUP_COL, \"description\"]).copy()\n",
    "\n",
    "print(\"Labeling rows (relevant==1):\", len(df_labels))\n",
    "print(\"Unique demand_id:\", df_labels[DEMAND_COL].nunique(), \"| groups:\", df_labels[GROUP_COL].nunique())\n",
    "\n",
    "train_lab, val_lab = train_test_split(\n",
    "    df_labels, test_size=TEST_SIZE_LABELS, random_state=SEED,\n",
    "    stratify=df_labels[GROUP_COL] if df_labels[GROUP_COL].nunique() > 1 else None\n",
    ")\n",
    "print(\"Train_lab:\", train_lab.shape, \"Val_lab:\", val_lab.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3663d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage A for labeling: Group model (TF-IDF + LinearSVC calibrated)\n",
    "group_pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(ngram_range=(1,3), min_df=2, max_df=0.95)),\n",
    "    (\"clf\", LinearSVC(class_weight=\"balanced\", max_iter=8000)),\n",
    "])\n",
    "group_pipe.fit(train_lab[TEXT_COL].astype(str), train_lab[GROUP_COL].astype(str))\n",
    "\n",
    "group_model = CalibratedClassifierCV(group_pipe, method=\"sigmoid\", cv=3)\n",
    "group_model.fit(train_lab[TEXT_COL].astype(str), train_lab[GROUP_COL].astype(str))\n",
    "\n",
    "joblib.dump(group_model, ARTIFACTS_DIR / \"group_model.joblib\")\n",
    "print(\"Saved:\", ARTIFACTS_DIR / \"group_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05af6c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build group -> label candidates: list of (demand_id, description)\n",
    "labels_rel = labels_df.copy()\n",
    "labels_rel[DEMAND_COL] = labels_rel[DEMAND_COL].astype(str)\n",
    "if GROUP_COL not in labels_rel.columns:\n",
    "    # derive groups from df_labels if labels.csv doesn't have group_id\n",
    "    labels_rel = labels_rel.merge(df_labels[[DEMAND_COL, GROUP_COL]].drop_duplicates(), on=DEMAND_COL, how=\"left\")\n",
    "labels_rel = labels_rel.dropna(subset=[GROUP_COL, \"description\"]).copy()\n",
    "labels_rel[GROUP_COL] = labels_rel[GROUP_COL].astype(str)\n",
    "labels_rel = labels_rel.drop_duplicates(subset=[DEMAND_COL], keep=\"first\")\n",
    "\n",
    "group_to_labels = {\n",
    "    g: list(zip(sub[DEMAND_COL].tolist(), sub[\"description\"].astype(str).tolist()))\n",
    "    for g, sub in labels_rel.groupby(GROUP_COL)\n",
    "}\n",
    "print(\"Groups with candidates:\", len(group_to_labels))\n",
    "print(\"Example:\", list(group_to_labels.items())[:1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922d0783",
   "metadata": {},
   "source": [
    "### 3A) Model A — Description-aware Cross‑Encoder + Top‑N groups + Hard Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb30a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNG = np.random.default_rng(SEED)\n",
    "\n",
    "def make_random_pairs(df_part: pd.DataFrame, neg_per_pos: int) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for _, r in df_part.iterrows():\n",
    "        text = str(r[TEXT_COL])\n",
    "        demand = str(r[DEMAND_COL])\n",
    "        group = str(r[GROUP_COL])\n",
    "        desc_pos = str(r[\"description\"])\n",
    "\n",
    "        rows.append({\"text\": text, \"description\": desc_pos, \"labels\": 1, \"group\": group, \"true_demand\": demand})\n",
    "\n",
    "        cand = group_to_labels.get(group, [])\n",
    "        neg_pool = [(d, desc) for d, desc in cand if d != demand]\n",
    "        if not neg_pool:\n",
    "            continue\n",
    "        take = min(neg_per_pos, len(neg_pool))\n",
    "        idx = RNG.choice(len(neg_pool), size=take, replace=False)\n",
    "        for i in np.atleast_1d(idx):\n",
    "            _, desc_neg = neg_pool[int(i)]\n",
    "            rows.append({\"text\": text, \"description\": str(desc_neg), \"labels\": 0, \"group\": group, \"true_demand\": demand})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "ce_train_pairs = make_random_pairs(train_lab, CE_NEG_RANDOM)\n",
    "ce_val_pairs   = make_random_pairs(val_lab, CE_NEG_RANDOM)\n",
    "\n",
    "print(\"CE train pairs:\", ce_train_pairs.shape, \"pos rate:\", ce_train_pairs[\"labels\"].mean())\n",
    "print(\"CE val pairs  :\", ce_val_pairs.shape, \"pos rate:\", ce_val_pairs[\"labels\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a62f814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Cross-Encoder\n",
    "ce_tokenizer = AutoTokenizer.from_pretrained(CE_MODEL)\n",
    "\n",
    "def ce_tok(batch):\n",
    "    return ce_tokenizer(\n",
    "        batch[\"text\"], batch[\"description\"],\n",
    "        truncation=True, padding=\"max_length\", max_length=CE_MAX_LEN\n",
    "    )\n",
    "\n",
    "def to_hfds(pairs_df: pd.DataFrame) -> Dataset:\n",
    "    ds = Dataset.from_pandas(pairs_df[[\"text\",\"description\",\"labels\"]], preserve_index=False)\n",
    "    ds = ds.map(ce_tok, batched=True)\n",
    "    ds.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n",
    "    return ds\n",
    "\n",
    "ce_train_ds = to_hfds(ce_train_pairs)\n",
    "ce_val_ds   = to_hfds(ce_val_pairs)\n",
    "\n",
    "ce_model = AutoModelForSequenceClassification.from_pretrained(CE_MODEL, num_labels=2).to(DEVICE)\n",
    "\n",
    "ce_args = TrainingArguments(\n",
    "    output_dir=str(ARTIFACTS_DIR / \"ce_out\"),\n",
    "    learning_rate=CE_LR,\n",
    "    per_device_train_batch_size=CE_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=CE_BATCH_SIZE,\n",
    "    num_train_epochs=CE_EPOCHS_BASE,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "ce_trainer = Trainer(model=ce_model, args=ce_args, train_dataset=ce_train_ds, eval_dataset=ce_val_ds)\n",
    "t0 = time.time()\n",
    "ce_trainer.train()\n",
    "print(\"CE base training seconds:\", round(time.time() - t0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a147a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def ce_score_probs(text: str, descriptions: list[str]) -> np.ndarray:\n",
    "    batch = ce_tokenizer([text]*len(descriptions), descriptions, padding=True, truncation=True, max_length=CE_MAX_LEN, return_tensors=\"pt\")\n",
    "    batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "    logits = ce_model(**batch).logits\n",
    "    return F.softmax(logits, dim=-1)[:, 1].detach().cpu().numpy()\n",
    "\n",
    "def topn_groups(texts: pd.Series, n: int) -> np.ndarray:\n",
    "    probs = group_model.predict_proba(texts.astype(str))\n",
    "    classes = group_model.classes_\n",
    "    topi = np.argsort(-probs, axis=1)[:, :n]\n",
    "    return classes[topi], probs[np.arange(len(probs))[:,None], topi]\n",
    "\n",
    "def ce_predict_topk_union(text: str, groups: list[str], k: int) -> list[str]:\n",
    "    cand = []\n",
    "    for g in groups:\n",
    "        cand.extend(group_to_labels.get(str(g), []))\n",
    "    if not cand:\n",
    "        return []\n",
    "    # dedupe demand_id\n",
    "    seen=set(); dids=[]; descs=[]\n",
    "    for did, desc in cand:\n",
    "        if did in seen: \n",
    "            continue\n",
    "        seen.add(did); dids.append(did); descs.append(desc)\n",
    "    scores = ce_score_probs(text, descs)\n",
    "    order = np.argsort(-scores)[:k]\n",
    "    return [dids[i] for i in order]\n",
    "\n",
    "def eval_ce(df_part: pd.DataFrame, top_n_groups: int, top_k: int):\n",
    "    groups_topn, _ = topn_groups(df_part[TEXT_COL], top_n_groups)\n",
    "    y_true = df_part[DEMAND_COL].astype(str).tolist()\n",
    "    texts  = df_part[TEXT_COL].astype(str).tolist()\n",
    "\n",
    "    top1=0; topk=0; valid=0\n",
    "    for text, y, gs in zip(texts, y_true, groups_topn):\n",
    "        preds = ce_predict_topk_union(text, list(gs), top_k)\n",
    "        if not preds:\n",
    "            continue\n",
    "        valid += 1\n",
    "        top1 += int(preds[0] == y)\n",
    "        topk += int(y in preds)\n",
    "    return {\"eval_rows\": valid, \"top1\": top1/max(1,valid), f\"top{top_k}\": topk/max(1,valid)}\n",
    "\n",
    "ce_metrics_base_top1 = eval_ce(val_lab, top_n_groups=1, top_k=TOP_K_LABELS)\n",
    "ce_metrics_base_topN = eval_ce(val_lab, top_n_groups=TOP_N_GROUPS, top_k=TOP_K_LABELS)\n",
    "print(\"CE base top-1 group:\", ce_metrics_base_top1)\n",
    "print(f\"CE base top-{TOP_N_GROUPS} groups:\", ce_metrics_base_topN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3536c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard-negative mining + short fine-tune\n",
    "def mine_hard_pairs(df_part: pd.DataFrame, hard_neg_per_pos: int) -> pd.DataFrame:\n",
    "    out = []\n",
    "    for _, r in df_part.iterrows():\n",
    "        text = str(r[TEXT_COL])\n",
    "        true_demand = str(r[DEMAND_COL])\n",
    "        group = str(r[GROUP_COL])\n",
    "\n",
    "        cand = group_to_labels.get(group, [])\n",
    "        if not cand:\n",
    "            continue\n",
    "\n",
    "        # optional cap for speed\n",
    "        if MAX_CANDIDATES_PER_GROUP_MINING is not None and len(cand) > MAX_CANDIDATES_PER_GROUP_MINING:\n",
    "            idx = RNG.choice(len(cand), size=MAX_CANDIDATES_PER_GROUP_MINING, replace=False)\n",
    "            cand = [cand[int(i)] for i in idx]\n",
    "\n",
    "        dids = [d for d, _ in cand]\n",
    "        descs = [desc for _, desc in cand]\n",
    "        scores = ce_score_probs(text, descs)\n",
    "        order = np.argsort(-scores)\n",
    "\n",
    "        # positive\n",
    "        out.append({\"text\": text, \"description\": str(r[\"description\"]), \"labels\": 1})\n",
    "\n",
    "        # hard negatives: highest scoring incorrect labels\n",
    "        taken = 0\n",
    "        for i in order:\n",
    "            if dids[int(i)] == true_demand:\n",
    "                continue\n",
    "            out.append({\"text\": text, \"description\": str(descs[int(i)]), \"labels\": 0})\n",
    "            taken += 1\n",
    "            if taken >= hard_neg_per_pos:\n",
    "                break\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "hard_pairs = mine_hard_pairs(val_lab, CE_NEG_HARD)\n",
    "print(\"Hard pairs:\", hard_pairs.shape, \"pos rate:\", hard_pairs[\"labels\"].mean())\n",
    "\n",
    "ft_pairs = pd.concat([ce_train_pairs[[\"text\",\"description\",\"labels\"]], hard_pairs], ignore_index=True)\n",
    "ft_ds = to_hfds(ft_pairs)\n",
    "\n",
    "ce_args_hard = TrainingArguments(\n",
    "    output_dir=str(ARTIFACTS_DIR / \"ce_out_hard\"),\n",
    "    learning_rate=CE_LR,\n",
    "    per_device_train_batch_size=CE_BATCH_SIZE,\n",
    "    num_train_epochs=CE_EPOCHS_HARD,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    ")\n",
    "ce_trainer_hard = Trainer(model=ce_model, args=ce_args_hard, train_dataset=ft_ds)\n",
    "t0 = time.time()\n",
    "ce_trainer_hard.train()\n",
    "print(\"CE hard fine-tune seconds:\", round(time.time() - t0, 1))\n",
    "\n",
    "ce_metrics_hard_top1 = eval_ce(val_lab, top_n_groups=1, top_k=TOP_K_LABELS)\n",
    "ce_metrics_hard_topN = eval_ce(val_lab, top_n_groups=TOP_N_GROUPS, top_k=TOP_K_LABELS)\n",
    "print(\"CE hard top-1 group:\", ce_metrics_hard_top1)\n",
    "print(f\"CE hard top-{TOP_N_GROUPS} groups:\", ce_metrics_hard_topN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c1212d",
   "metadata": {},
   "source": [
    "### 3B) Model B — Hierarchical Transformer (shared encoder + per-group head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6eece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build per-group label list FROM TRAIN ONLY (avoid leakage)\n",
    "train_groups = train_lab[GROUP_COL].astype(str)\n",
    "\n",
    "group_label_list = {}\n",
    "for g, sub in train_lab.groupby(GROUP_COL):\n",
    "    group_label_list[str(g)] = sorted(sub[DEMAND_COL].astype(str).unique().tolist())\n",
    "\n",
    "group_label_to_idx = {g: {lab:i for i, lab in enumerate(labs)} for g, labs in group_label_list.items()}\n",
    "group_num_labels = {g: len(labs) for g, labs in group_label_list.items()}\n",
    "\n",
    "# Keep only validation rows whose label exists in train within the same group\n",
    "def label_seen(row):\n",
    "    g = str(row[GROUP_COL]); lab = str(row[DEMAND_COL])\n",
    "    return g in group_label_to_idx and lab in group_label_to_idx[g]\n",
    "\n",
    "val_seen = val_lab[val_lab.apply(label_seen, axis=1)].copy()\n",
    "print(\"Val rows:\", len(val_lab), \"| val rows with label seen in train:\", len(val_seen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b827eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_tokenizer = AutoTokenizer.from_pretrained(HT_MODEL)\n",
    "\n",
    "def ht_tok(batch):\n",
    "    return ht_tokenizer(batch[TEXT_COL], truncation=True, padding=\"max_length\", max_length=HT_MAX_LEN)\n",
    "\n",
    "def to_examples(df_part: pd.DataFrame) -> pd.DataFrame:\n",
    "    ex = df_part[[TEXT_COL, GROUP_COL, DEMAND_COL]].copy()\n",
    "    ex[GROUP_COL] = ex[GROUP_COL].astype(str)\n",
    "    ex[DEMAND_COL] = ex[DEMAND_COL].astype(str)\n",
    "    ex[\"group\"] = ex[GROUP_COL]\n",
    "    ex[\"label_local\"] = ex.apply(lambda r: group_label_to_idx[str(r[GROUP_COL])][str(r[DEMAND_COL])], axis=1)\n",
    "    return ex[[TEXT_COL, \"group\", \"label_local\"]]\n",
    "\n",
    "ht_train_ex = to_examples(train_lab)\n",
    "ht_val_ex   = to_examples(val_seen)\n",
    "\n",
    "train_ds = Dataset.from_pandas(ht_train_ex, preserve_index=False).map(ht_tok, batched=True)\n",
    "val_ds   = Dataset.from_pandas(ht_val_ex, preserve_index=False).map(ht_tok, batched=True)\n",
    "\n",
    "train_ds.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label_local\"])\n",
    "val_ds.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label_local\"])\n",
    "\n",
    "train_groups_list = ht_train_ex[\"group\"].tolist()\n",
    "val_groups_list   = ht_val_ex[\"group\"].tolist()\n",
    "\n",
    "def build_torch_dataset(ds: Dataset, groups: list[str]):\n",
    "    return list(zip(ds[\"input_ids\"], ds[\"attention_mask\"], ds[\"label_local\"], groups))\n",
    "\n",
    "train_torch = build_torch_dataset(train_ds, train_groups_list)\n",
    "val_torch   = build_torch_dataset(val_ds, val_groups_list)\n",
    "\n",
    "def collate(batch):\n",
    "    return {\n",
    "        \"input_ids\": torch.stack([b[0] for b in batch]),\n",
    "        \"attention_mask\": torch.stack([b[1] for b in batch]),\n",
    "        \"labels\": torch.stack([b[2] for b in batch]),\n",
    "        \"groups\": [b[3] for b in batch],\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(train_torch, batch_size=HT_BATCH_SIZE, shuffle=True, collate_fn=collate)\n",
    "val_loader   = DataLoader(val_torch, batch_size=HT_BATCH_SIZE, shuffle=False, collate_fn=collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d37fd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierTransformer(nn.Module):\n",
    "    def __init__(self, base_model_name: str, group_num_labels: dict[str,int], class_weights: dict[str, torch.Tensor] | None = None):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(base_model_name)\n",
    "        hidden = self.encoder.config.hidden_size\n",
    "        self.heads = nn.ModuleDict({g: nn.Linear(hidden, n) for g, n in group_num_labels.items()})\n",
    "        self.class_weights = class_weights or {}\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, groups, labels=None):\n",
    "        enc = self.encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0]\n",
    "        losses = []\n",
    "        logits_per_example = []\n",
    "\n",
    "        for i in range(enc.size(0)):\n",
    "            g = groups[i]\n",
    "            logits = self.heads[g](enc[i])  # [n_labels_in_group]\n",
    "            logits_per_example.append(logits)\n",
    "\n",
    "            if labels is not None:\n",
    "                w = self.class_weights.get(g, None)\n",
    "                loss_fn = nn.CrossEntropyLoss(weight=w.to(logits.device) if w is not None else None)\n",
    "                losses.append(loss_fn(logits.unsqueeze(0), labels[i].unsqueeze(0)))\n",
    "\n",
    "        loss = torch.stack(losses).mean() if losses else None\n",
    "        return loss, logits_per_example\n",
    "\n",
    "# Optional per-group class weights (inverse frequency)\n",
    "class_w = {}\n",
    "if HT_USE_CLASS_WEIGHTS:\n",
    "    for g, sub in ht_train_ex.groupby(\"group\"):\n",
    "        counts = sub[\"label_local\"].value_counts().sort_index()\n",
    "        w = (counts.sum() / (counts + 1e-9)).values.astype(np.float32)\n",
    "        w = w / w.mean()\n",
    "        class_w[str(g)] = torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "ht_model = HierTransformer(HT_MODEL, group_num_labels, class_weights=class_w if HT_USE_CLASS_WEIGHTS else None).to(DEVICE)\n",
    "optim = torch.optim.AdamW(ht_model.parameters(), lr=HT_LR, weight_decay=HT_WEIGHT_DECAY)\n",
    "\n",
    "def train_epoch():\n",
    "    ht_model.train()\n",
    "    total=0.0; n=0\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        loss, _ = ht_model(\n",
    "            batch[\"input_ids\"].to(DEVICE),\n",
    "            batch[\"attention_mask\"].to(DEVICE),\n",
    "            batch[\"groups\"],\n",
    "            batch[\"labels\"].to(DEVICE),\n",
    "        )\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total += float(loss.detach().cpu())\n",
    "        n += 1\n",
    "    return total/max(1,n)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch():\n",
    "    ht_model.eval()\n",
    "    total=0.0; n=0\n",
    "    for batch in val_loader:\n",
    "        loss, _ = ht_model(\n",
    "            batch[\"input_ids\"].to(DEVICE),\n",
    "            batch[\"attention_mask\"].to(DEVICE),\n",
    "            batch[\"groups\"],\n",
    "            batch[\"labels\"].to(DEVICE),\n",
    "        )\n",
    "        total += float(loss.detach().cpu())\n",
    "        n += 1\n",
    "    return total/max(1,n)\n",
    "\n",
    "best = 1e18\n",
    "best_state = None\n",
    "t0 = time.time()\n",
    "for e in range(1, HT_EPOCHS+1):\n",
    "    tr = train_epoch()\n",
    "    va = eval_epoch()\n",
    "    print(f\"HT epoch {e}: train_loss={tr:.4f} val_loss={va:.4f}\")\n",
    "    if va < best:\n",
    "        best = va\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in ht_model.state_dict().items()}\n",
    "\n",
    "if best_state is not None:\n",
    "    ht_model.load_state_dict(best_state)\n",
    "\n",
    "print(\"HT training seconds:\", round(time.time() - t0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3b0528",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def ht_predict_topk_union(text: str, groups: list[str], group_probs: list[float], k: int) -> list[str]:\n",
    "    # Combine group prob with within-group label prob (simple but effective): score = P(group)*P(label|group)\n",
    "    token = ht_tokenizer(text, truncation=True, padding=\"max_length\", max_length=HT_MAX_LEN, return_tensors=\"pt\")\n",
    "    token = {k: v.to(DEVICE) for k, v in token.items()}\n",
    "    enc = ht_model.encoder(**token).last_hidden_state[:, 0][0]\n",
    "\n",
    "    cand = []\n",
    "    for g, pg in zip(groups, group_probs):\n",
    "        g = str(g)\n",
    "        if g not in group_label_list:\n",
    "            continue\n",
    "        logits = ht_model.heads[g](enc)\n",
    "        probs = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        labs = group_label_list[g]\n",
    "        # take top 10 from each group then merge\n",
    "        top_local = np.argsort(-probs)[: min(10, len(labs))]\n",
    "        for i in top_local:\n",
    "            cand.append((labs[int(i)], float(pg) * float(probs[int(i)])))\n",
    "\n",
    "    if not cand:\n",
    "        return []\n",
    "    # sort global and take top-k unique\n",
    "    cand.sort(key=lambda x: -x[1])\n",
    "    seen=set(); out=[]\n",
    "    for lab, _ in cand:\n",
    "        if lab in seen:\n",
    "            continue\n",
    "        seen.add(lab); out.append(lab)\n",
    "        if len(out) >= k:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "def eval_ht(df_part: pd.DataFrame, top_n_groups: int, top_k: int) -> dict:\n",
    "    groups_topn, probs_topn = topn_groups(df_part[TEXT_COL], top_n_groups)\n",
    "    y_true = df_part[DEMAND_COL].astype(str).tolist()\n",
    "    texts  = df_part[TEXT_COL].astype(str).tolist()\n",
    "\n",
    "    top1=0; topk=0; valid=0\n",
    "    for text, y, gs, ps in zip(texts, y_true, groups_topn, probs_topn):\n",
    "        preds = ht_predict_topk_union(text, list(gs), list(ps), top_k)\n",
    "        if not preds:\n",
    "            continue\n",
    "        valid += 1\n",
    "        top1 += int(preds[0] == y)\n",
    "        topk += int(y in preds)\n",
    "    return {\"eval_rows\": valid, \"top1\": top1/max(1,valid), f\"top{top_k}\": topk/max(1,valid)}\n",
    "\n",
    "ht_metrics_top1 = eval_ht(val_seen, top_n_groups=1, top_k=TOP_K_LABELS)\n",
    "ht_metrics_topN = eval_ht(val_seen, top_n_groups=TOP_N_GROUPS, top_k=TOP_K_LABELS)\n",
    "\n",
    "print(\"HT top-1 group:\", ht_metrics_top1)\n",
    "print(f\"HT top-{TOP_N_GROUPS} groups:\", ht_metrics_topN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946f1d03",
   "metadata": {},
   "source": [
    "## 4) Compare label models + choose best + save artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb4fbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metrics (use CE hard + top-N as the strongest CE variant)\n",
    "rows = []\n",
    "\n",
    "rows.append({\"model\": \"CE_base_top1group\", **ce_metrics_base_top1})\n",
    "rows.append({\"model\": f\"CE_base_top{TOP_N_GROUPS}groups\", **ce_metrics_base_topN})\n",
    "rows.append({\"model\": \"CE_hard_top1group\", **ce_metrics_hard_top1})\n",
    "rows.append({\"model\": f\"CE_hard_top{TOP_N_GROUPS}groups\", **ce_metrics_hard_topN})\n",
    "\n",
    "rows.append({\"model\": \"HT_top1group\", **ht_metrics_top1})\n",
    "rows.append({\"model\": f\"HT_top{TOP_N_GROUPS}groups\", **ht_metrics_topN})\n",
    "\n",
    "metrics = pd.DataFrame(rows)\n",
    "metrics_path = ARTIFACTS_DIR / \"comparison_metrics.csv\"\n",
    "metrics.to_csv(metrics_path, index=False)\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa1e872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Top-1 and Top-K bars\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(metrics[\"model\"], metrics[\"top1\"])\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Top-1 accuracy\")\n",
    "plt.title(\"Label model comparison — Top-1\")\n",
    "plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(metrics[\"model\"], metrics[f\"top{TOP_K_LABELS}\"])\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(f\"Top-{TOP_K_LABELS} accuracy\")\n",
    "plt.title(f\"Label model comparison — Top-{TOP_K_LABELS}\")\n",
    "plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8964a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best labeling model by Top-K (tie-break Top-1)\n",
    "metrics_sorted = metrics.sort_values([f\"top{TOP_K_LABELS}\", \"top1\"], ascending=False)\n",
    "best_label_variant = metrics_sorted.iloc[0][\"model\"]\n",
    "print(\"Best label variant:\", best_label_variant)\n",
    "\n",
    "# Save CE artifacts (always save, since it's commonly best)\n",
    "ce_save_dir = ARTIFACTS_DIR / \"cross_encoder\"\n",
    "ce_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "ce_model.save_pretrained(ce_save_dir)\n",
    "ce_tokenizer.save_pretrained(ce_save_dir)\n",
    "\n",
    "# Save mapping\n",
    "(ARTIFACTS_DIR / \"group_to_labels.json\").write_text(\n",
    "    json.dumps({g: [{\"demand_id\": d, \"description\": desc} for d, desc in pairs] for g, pairs in group_to_labels.items()}, indent=2),\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "# Save HT artifacts\n",
    "torch.save(ht_model.state_dict(), ARTIFACTS_DIR / \"hierarchical_transformer.pt\")\n",
    "meta = {\n",
    "    \"ht_model\": HT_MODEL,\n",
    "    \"group_label_list\": group_label_list,\n",
    "    \"ht_max_len\": HT_MAX_LEN,\n",
    "}\n",
    "(ARTIFACTS_DIR / \"hierarchical_transformer_meta.json\").write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Saved label artifacts to:\", ARTIFACTS_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b136517",
   "metadata": {},
   "source": [
    "## 5) What you deploy\n",
    "\n",
    "- **Relevance**: `artifacts/relevance_model.joblib` (model + threshold)\n",
    "- **Group model**: `artifacts/group_model.joblib`\n",
    "- **Label model**: usually `artifacts/cross_encoder/` + `group_to_labels.json` + `TOP_N_GROUPS`\n",
    "  - For speed: use `hierarchical_transformer.pt` + meta + group model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb33e169",
   "metadata": {},
   "source": [
    "## (Optional) Run this notebook on Azure ML (v2 SDK) — compute + environment + command job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad53fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is meant to be executed *locally* (e.g., on your laptop/Jupyter).\n",
    "# It submits an Azure ML command job that runs THIS notebook on a remote compute cluster via Papermill.\n",
    "# When the job runs remotely, AZUREML_RUN_ID is set, so we skip submission logic to avoid recursion.\n",
    "\n",
    "import os\n",
    "\n",
    "if os.getenv(\"AZUREML_RUN_ID\"):\n",
    "    print(\"Running inside Azure ML job; skipping job-submission cell.\")\n",
    "else:\n",
    "    # 1) Install deps (run once per local environment)\n",
    "    # %pip install -U azure-ai-ml azure-identity papermill\n",
    "\n",
    "    from azure.identity import DefaultAzureCredential\n",
    "    from azure.ai.ml import MLClient, command, Input\n",
    "    from azure.ai.ml.entities import AmlCompute, Environment\n",
    "    from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "    # 2) Auth + workspace\n",
    "    # Preferred: config.json in the same folder (created by `az ml folder attach -w <ws> -g <rg>`),\n",
    "    # or set env vars: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP, AZURE_WORKSPACE_NAME\n",
    "    try:\n",
    "        ml_client = MLClient.from_config(credential=DefaultAzureCredential(exclude_shared_token_cache_credential=True))\n",
    "        print(\"Loaded workspace from config.json\")\n",
    "    except Exception:\n",
    "        sub_id = os.environ[\"AZURE_SUBSCRIPTION_ID\"]\n",
    "        rg     = os.environ[\"AZURE_RESOURCE_GROUP\"]\n",
    "        ws     = os.environ[\"AZURE_WORKSPACE_NAME\"]\n",
    "        ml_client = MLClient(DefaultAzureCredential(exclude_shared_token_cache_credential=True), sub_id, rg, ws)\n",
    "        print(\"Loaded workspace from env vars\")\n",
    "\n",
    "    # 3) Compute\n",
    "    compute_name = os.getenv(\"AML_COMPUTE_NAME\", \"cpu-cluster\")\n",
    "    try:\n",
    "        ml_client.compute.get(compute_name)\n",
    "        print(f\"Compute '{compute_name}' exists.\")\n",
    "    except Exception:\n",
    "        cpu_cluster = AmlCompute(\n",
    "            name=compute_name,\n",
    "            type=\"amlcompute\",\n",
    "            size=os.getenv(\"AML_VM_SIZE\", \"STANDARD_D4_V3\"),\n",
    "            min_instances=int(os.getenv(\"AML_MIN_INSTANCES\", \"0\")),\n",
    "            max_instances=int(os.getenv(\"AML_MAX_INSTANCES\", \"2\")),\n",
    "            idle_time_before_scale_down=120,\n",
    "        )\n",
    "        ml_client.compute.begin_create_or_update(cpu_cluster).result()\n",
    "        print(f\"Created compute '{compute_name}'.\")\n",
    "\n",
    "    # 4) Environment (conda)\n",
    "    env_name = os.getenv(\"AML_ENV_NAME\", \"nlp-pipeline-env\")\n",
    "    env_version = os.getenv(\"AML_ENV_VERSION\", \"1\")\n",
    "    conda_yaml = \"\"\"name: nlp-pipeline\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.10\n",
    "  - pip\n",
    "  - pip:\n",
    "      - pandas==2.2.2\n",
    "      - numpy==1.26.4\n",
    "      - scikit-learn==1.5.1\n",
    "      - matplotlib==3.9.0\n",
    "      - joblib==1.4.2\n",
    "      - torch==2.3.1\n",
    "      - transformers==4.44.2\n",
    "      - datasets==2.20.0\n",
    "      - accelerate==0.33.0\n",
    "      - sentence-transformers==3.0.1\n",
    "      - lightgbm==4.5.0\n",
    "      - papermill==2.6.0\n",
    "\"\"\"\n",
    "\n",
    "    env = Environment(\n",
    "        name=env_name,\n",
    "        version=env_version,\n",
    "        description=\"Env for full_pipeline_end_to_end.ipynb (Azure ML v2)\",\n",
    "        conda_file=conda_yaml,\n",
    "        image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
    "    )\n",
    "    env = ml_client.environments.create_or_update(env)\n",
    "    print(\"Registered env:\", env.name, env.version)\n",
    "\n",
    "    # 5) Submit a command job that executes the notebook with Papermill\n",
    "    # Inputs are auto-uploaded from local paths.\n",
    "    job = command(\n",
    "        code=\".\",  # folder that contains this notebook and the CSVs\n",
    "        command=(\n",
    "            \"papermill full_pipeline_end_to_end.ipynb outputs/executed.ipynb \"\n",
    "            \"-p TRAIN_CSV ${{inputs.train_csv}} \"\n",
    "            \"-p TEST_CSV ${{inputs.test_csv}} \"\n",
    "            \"-p LABELS_CSV ${{inputs.labels_csv}} \"\n",
    "            \"-p OUTPUT_DIR outputs\"\n",
    "        ),\n",
    "        inputs={\n",
    "            \"train_csv\": Input(type=AssetTypes.URI_FILE, path=str(TRAIN_CSV)),\n",
    "            \"test_csv\":  Input(type=AssetTypes.URI_FILE, path=str(TEST_CSV)),\n",
    "            \"labels_csv\":Input(type=AssetTypes.URI_FILE, path=str(LABELS_CSV)),\n",
    "        },\n",
    "        environment=f\"{env.name}:{env.version}\",\n",
    "        compute=compute_name,\n",
    "        display_name=\"nlp-full-pipeline-notebook\",\n",
    "        experiment_name=os.getenv(\"AML_EXPERIMENT_NAME\", \"nlp_full_pipeline\"),\n",
    "    )\n",
    "\n",
    "    returned_job = ml_client.jobs.create_or_update(job)\n",
    "    print(\"Submitted job:\", returned_job.name)\n",
    "    print(\"Studio URL:\", returned_job.studio_url)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
