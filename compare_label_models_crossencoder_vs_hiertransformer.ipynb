{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc7b459b",
   "metadata": {},
   "source": [
    "# Compare label classification solutions (relevant==1)\n",
    "\n",
    "This notebook compares **two label models** on the **same train/val split** (relevant-only):\n",
    "\n",
    "A) **Hierarchical + Description-aware Cross-Encoder**\n",
    "- Stage A: TF‑IDF + LinearSVC → `group_id`\n",
    "- Stage B: Cross‑Encoder (text + label description) → scores labels *within the predicted group*\n",
    "- Report: Top‑1 / Top‑5 accuracy (and optional oracle-group upper bound)\n",
    "\n",
    "B) **Hierarchical Transformer (shared encoder + group-specific heads)**\n",
    "- Stage A: TF‑IDF + LinearSVC → `group_id` (same as A for fairness)\n",
    "- Stage B: Transformer encoder + **per-group head** → predicts label within predicted group\n",
    "- Report: Top‑1 / Top‑5 accuracy\n",
    "\n",
    "> Notes:\n",
    "> - Cross‑encoder usually wins on very fine distinctions but is slower at inference.\n",
    "> - Hierarchical transformer is faster at inference but can be slightly less accurate on ultra-similar labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0bb658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CONFIG (EDIT THESE)\n",
    "# =========================\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_CSV = Path(\"dataset.csv\")   # must include text, demand_id, relevant/relevance, group_id (or mergeable)\n",
    "LABELS_CSV  = Path(\"labels.csv\")    # demand_id, description, (optional) group_id\n",
    "\n",
    "TEXT_COL   = \"text\"\n",
    "DEMAND_COL = \"demand_id\"\n",
    "GROUP_COL  = \"group_id\"\n",
    "REL_COL_CANDIDATES = [\"relevant\", \"relevance\"]\n",
    "\n",
    "SEED = 42\n",
    "TEST_SIZE = 0.30\n",
    "TOP_K = 5\n",
    "\n",
    "# Stage A (group model)\n",
    "TFIDF_NGRAM = (1,3)\n",
    "\n",
    "# Cross-Encoder (A)\n",
    "CE_MODEL = \"microsoft/deberta-v3-base\"\n",
    "CE_MAX_LEN = 256\n",
    "CE_EPOCHS = 2\n",
    "CE_BS = 8\n",
    "CE_LR = 2e-5\n",
    "CE_NEG_PER_POS = 3\n",
    "\n",
    "# Hierarchical Transformer (B)\n",
    "HT_MODEL = \"microsoft/deberta-v3-base\"\n",
    "HT_MAX_LEN = 256\n",
    "HT_EPOCHS = 3\n",
    "HT_BS = 8\n",
    "HT_LR = 2e-5\n",
    "HT_WEIGHT_DECAY = 0.01\n",
    "USE_CLASS_WEIGHTS = True  # per-group class weights to help long tail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1267e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModel,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RNG = np.random.default_rng(SEED)\n",
    "\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5051d12",
   "metadata": {},
   "source": [
    "## 1) Load + merge + keep relevant==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775a06f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET_CSV)\n",
    "labels = pd.read_csv(LABELS_CSV)\n",
    "\n",
    "# detect relevance column\n",
    "rel_col = None\n",
    "for c in REL_COL_CANDIDATES:\n",
    "    if c in df.columns:\n",
    "        rel_col = c\n",
    "        break\n",
    "if rel_col is None:\n",
    "    raise ValueError(f\"Missing relevance column. Tried: {REL_COL_CANDIDATES}. Have: {list(df.columns)}\")\n",
    "\n",
    "# required\n",
    "for c in [TEXT_COL, DEMAND_COL]:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f\"Dataset missing column: {c}\")\n",
    "for c in [DEMAND_COL, \"description\"]:\n",
    "    if c not in labels.columns:\n",
    "        raise ValueError(f\"Labels file missing column: {c}\")\n",
    "\n",
    "df[rel_col] = df[rel_col].fillna(0).astype(int)\n",
    "df[DEMAND_COL] = df[DEMAND_COL].astype(str)\n",
    "labels[DEMAND_COL] = labels[DEMAND_COL].astype(str)\n",
    "\n",
    "# bring group_id if missing\n",
    "if GROUP_COL not in df.columns and GROUP_COL in labels.columns:\n",
    "    df = df.merge(labels[[DEMAND_COL, GROUP_COL]], on=DEMAND_COL, how=\"left\")\n",
    "if GROUP_COL not in df.columns:\n",
    "    raise ValueError(f\"Missing {GROUP_COL}. Provide in dataset.csv or labels.csv and set GROUP_COL.\")\n",
    "\n",
    "# merge descriptions\n",
    "df = df.merge(labels[[DEMAND_COL, \"description\"]], on=DEMAND_COL, how=\"left\")\n",
    "\n",
    "df_rel = df[df[rel_col] == 1].dropna(subset=[TEXT_COL, DEMAND_COL, GROUP_COL, \"description\"]).copy()\n",
    "df_rel[GROUP_COL] = df_rel[GROUP_COL].astype(str)\n",
    "\n",
    "print(\"Relevant rows:\", len(df_rel))\n",
    "print(\"Unique labels:\", df_rel[DEMAND_COL].nunique(), \"Unique groups:\", df_rel[GROUP_COL].nunique())\n",
    "df_rel.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10ace87",
   "metadata": {},
   "source": [
    "## 2) Single shared train/val split (stratify by group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b1213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    df_rel,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=SEED,\n",
    "    stratify=df_rel[GROUP_COL] if df_rel[GROUP_COL].nunique() > 1 else None,\n",
    ")\n",
    "\n",
    "print(\"Train:\", train_df.shape, \"Val:\", val_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a30a76",
   "metadata": {},
   "source": [
    "## 3) Stage A — Group classifier (shared for both solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0321137",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "group_pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(min_df=2, max_df=0.95, ngram_range=TFIDF_NGRAM)),\n",
    "    (\"clf\", LinearSVC(class_weight=\"balanced\", max_iter=8000)),\n",
    "])\n",
    "group_pipe.fit(train_df[TEXT_COL].astype(str), train_df[GROUP_COL])\n",
    "\n",
    "group_cal = CalibratedClassifierCV(group_pipe, method=\"sigmoid\", cv=3)\n",
    "group_cal.fit(train_df[TEXT_COL].astype(str), train_df[GROUP_COL])\n",
    "\n",
    "pred_groups = group_cal.predict(val_df[TEXT_COL].astype(str))\n",
    "print(classification_report(val_df[GROUP_COL], pred_groups, zero_division=0))\n",
    "print(\"Stage A trained in %.1fs\" % (time.time() - t0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3432f7a",
   "metadata": {},
   "source": [
    "## 4) Prepare label candidates per group (demand_id + description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b7b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_rel = labels.merge(df_rel[[DEMAND_COL, GROUP_COL]].drop_duplicates(), on=DEMAND_COL, how=\"inner\")\n",
    "labels_rel = labels_rel.dropna(subset=[GROUP_COL, \"description\"]).copy()\n",
    "labels_rel[GROUP_COL] = labels_rel[GROUP_COL].astype(str)\n",
    "\n",
    "group_to_labels = {}\n",
    "for g, sub in labels_rel.groupby(GROUP_COL):\n",
    "    pairs = list(zip(sub[DEMAND_COL].astype(str).tolist(), sub[\"description\"].astype(str).tolist()))\n",
    "    seen=set(); uniq=[]\n",
    "    for did, desc in pairs:\n",
    "        if did in seen: \n",
    "            continue\n",
    "        seen.add(did)\n",
    "        uniq.append((did, desc))\n",
    "    group_to_labels[str(g)] = uniq\n",
    "\n",
    "print(\"Groups with candidates:\", len(group_to_labels))\n",
    "print(\"Example group sizes:\", sorted([(g, len(v)) for g, v in group_to_labels.items()], key=lambda x: -x[1])[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb752ab",
   "metadata": {},
   "source": [
    "# A) Cross‑Encoder (text + description)\n",
    "\n",
    "We train a binary scorer on pairs:\n",
    "- positive: (text, correct description)\n",
    "- negative: (text, other description within same group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58035fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairs(df_part: pd.DataFrame, neg_per_pos: int) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for _, r in df_part.iterrows():\n",
    "        text = str(r[TEXT_COL])\n",
    "        demand = str(r[DEMAND_COL])\n",
    "        group = str(r[GROUP_COL])\n",
    "        desc_pos = str(r[\"description\"])\n",
    "\n",
    "        rows.append({\"text\": text, \"description\": desc_pos, \"labels\": 1, \"group\": group, \"true_demand\": demand})\n",
    "\n",
    "        candidates = group_to_labels.get(group, [])\n",
    "        neg_pool = [(d, desc) for d, desc in candidates if d != demand]\n",
    "        if not neg_pool:\n",
    "            continue\n",
    "        take = min(neg_per_pos, len(neg_pool))\n",
    "        neg_idx = RNG.choice(len(neg_pool), size=take, replace=False)\n",
    "        for i in np.atleast_1d(neg_idx):\n",
    "            _, desc_neg = neg_pool[int(i)]\n",
    "            rows.append({\"text\": text, \"description\": str(desc_neg), \"labels\": 0, \"group\": group, \"true_demand\": demand})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "ce_train_pairs = make_pairs(train_df, CE_NEG_PER_POS)\n",
    "ce_val_pairs = make_pairs(val_df, CE_NEG_PER_POS)\n",
    "\n",
    "print(\"CE pairs train:\", ce_train_pairs.shape, \"pos rate:\", ce_train_pairs[\"labels\"].mean())\n",
    "print(\"CE pairs val  :\", ce_val_pairs.shape, \"pos rate:\", ce_val_pairs[\"labels\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d96a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train cross-encoder\n",
    "t0 = time.time()\n",
    "\n",
    "ce_tokenizer = AutoTokenizer.from_pretrained(CE_MODEL)\n",
    "\n",
    "def ce_tok(batch):\n",
    "    return ce_tokenizer(\n",
    "        batch[\"text\"],\n",
    "        batch[\"description\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=CE_MAX_LEN,\n",
    "    )\n",
    "\n",
    "ce_train_ds = Dataset.from_pandas(ce_train_pairs[[\"text\",\"description\",\"labels\"]], preserve_index=False).map(ce_tok, batched=True)\n",
    "ce_val_ds   = Dataset.from_pandas(ce_val_pairs[[\"text\",\"description\",\"labels\"]], preserve_index=False).map(ce_tok, batched=True)\n",
    "\n",
    "ce_train_ds.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n",
    "ce_val_ds.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n",
    "\n",
    "ce_model = AutoModelForSequenceClassification.from_pretrained(CE_MODEL, num_labels=2).to(device)\n",
    "\n",
    "ce_args = TrainingArguments(\n",
    "    output_dir=\"ce_compare_out\",\n",
    "    learning_rate=CE_LR,\n",
    "    per_device_train_batch_size=CE_BS,\n",
    "    per_device_eval_batch_size=CE_BS,\n",
    "    num_train_epochs=CE_EPOCHS,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "ce_trainer = Trainer(model=ce_model, args=ce_args, train_dataset=ce_train_ds, eval_dataset=ce_val_ds)\n",
    "ce_trainer.train()\n",
    "\n",
    "print(\"Cross-encoder trained in %.1fs\" % (time.time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4a90ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate cross-encoder demand_id Top-1 / Top-K on validation, using predicted group (fair)\n",
    "@torch.no_grad()\n",
    "def ce_score_probs(text: str, descriptions: list[str]) -> np.ndarray:\n",
    "    batch = ce_tokenizer([text]*len(descriptions), descriptions, padding=True, truncation=True, max_length=CE_MAX_LEN, return_tensors=\"pt\")\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    logits = ce_model(**batch).logits\n",
    "    probs = F.softmax(logits, dim=-1)[:, 1].detach().cpu().numpy()\n",
    "    return probs\n",
    "\n",
    "def ce_predict_topk(text: str, group: str, k: int) -> list[str]:\n",
    "    pairs = group_to_labels.get(str(group), [])\n",
    "    if not pairs:\n",
    "        return []\n",
    "    demand_ids = [d for d, _ in pairs]\n",
    "    descs = [desc for _, desc in pairs]\n",
    "    scores = ce_score_probs(text, descs)\n",
    "    order = np.argsort(-scores)\n",
    "    return [demand_ids[i] for i in order[:k]]\n",
    "\n",
    "# group prediction\n",
    "group_probs = group_cal.predict_proba(val_df[TEXT_COL].astype(str))\n",
    "group_classes = group_cal.classes_\n",
    "val_top1_group = group_classes[np.argmax(group_probs, axis=1)]\n",
    "\n",
    "y_true = val_df[DEMAND_COL].astype(str).tolist()\n",
    "texts = val_df[TEXT_COL].astype(str).tolist()\n",
    "\n",
    "ce_top1 = 0\n",
    "ce_topk = 0\n",
    "valid = 0\n",
    "\n",
    "for text, true_lab, g in zip(texts, y_true, val_top1_group):\n",
    "    preds = ce_predict_topk(text, str(g), TOP_K)\n",
    "    if not preds:\n",
    "        continue\n",
    "    valid += 1\n",
    "    ce_top1 += int(preds[0] == true_lab)\n",
    "    ce_topk += int(true_lab in preds)\n",
    "\n",
    "ce_top1_acc = ce_top1 / max(1, valid)\n",
    "ce_topk_acc = ce_topk / max(1, valid)\n",
    "\n",
    "print(\"CE evaluated rows:\", valid, \"/\", len(val_df))\n",
    "print(\"CE Top-1:\", ce_top1_acc)\n",
    "print(f\"CE Top-{TOP_K}:\", ce_topk_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666e74a7",
   "metadata": {},
   "source": [
    "# B) Hierarchical Transformer (shared encoder + per-group head)\n",
    "\n",
    "We train a transformer that predicts the fine label **within the group**.\n",
    "To keep it correct and simple, we:\n",
    "- encode labels *per group* (local indices)\n",
    "- use **per-example loss** with the correct group head\n",
    "- at inference, we use the **predicted group** from Stage A (same as CE for fairness).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aded8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build per-group label index maps from TRAIN ONLY (avoid leakage)\n",
    "train_groups = train_df[GROUP_COL].astype(str).tolist()\n",
    "\n",
    "group_label_list = {}\n",
    "for g, sub in train_df.groupby(GROUP_COL):\n",
    "    uniq = sorted(sub[DEMAND_COL].astype(str).unique().tolist())\n",
    "    group_label_list[str(g)] = uniq\n",
    "\n",
    "# Map (group, demand_id) -> local index\n",
    "group_label_to_idx = {g: {lab:i for i, lab in enumerate(labs)} for g, labs in group_label_list.items()}\n",
    "group_num_labels = {g: len(labs) for g, labs in group_label_list.items()}\n",
    "\n",
    "# Filter val rows whose label is unseen in train within that group (can't be predicted)\n",
    "def is_seen(row):\n",
    "    g = str(row[GROUP_COL])\n",
    "    lab = str(row[DEMAND_COL])\n",
    "    return g in group_label_to_idx and lab in group_label_to_idx[g]\n",
    "\n",
    "val_seen_df = val_df[val_df.apply(is_seen, axis=1)].copy()\n",
    "\n",
    "print(\"Train groups:\", len(group_label_list))\n",
    "print(\"Val rows (all):\", len(val_df), \"Val rows (label seen in train):\", len(val_seen_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22564bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare HF datasets for hierarchical transformer training (train only)\n",
    "ht_tokenizer = AutoTokenizer.from_pretrained(HT_MODEL)\n",
    "\n",
    "def ht_tok(batch):\n",
    "    return ht_tokenizer(batch[TEXT_COL], truncation=True, padding=\"max_length\", max_length=HT_MAX_LEN)\n",
    "\n",
    "def to_examples(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = frame[[TEXT_COL, GROUP_COL, DEMAND_COL]].copy()\n",
    "    out[GROUP_COL] = out[GROUP_COL].astype(str)\n",
    "    out[DEMAND_COL] = out[DEMAND_COL].astype(str)\n",
    "    out[\"group\"] = out[GROUP_COL]\n",
    "    out[\"label_local\"] = out.apply(lambda r: group_label_to_idx[str(r[GROUP_COL])][str(r[DEMAND_COL])], axis=1)\n",
    "    return out[[TEXT_COL, \"group\", \"label_local\"]]\n",
    "\n",
    "ht_train_ex = to_examples(train_df)\n",
    "ht_val_ex = to_examples(val_seen_df)\n",
    "\n",
    "ht_train_ds = Dataset.from_pandas(ht_train_ex, preserve_index=False).map(ht_tok, batched=True)\n",
    "ht_val_ds   = Dataset.from_pandas(ht_val_ex, preserve_index=False).map(ht_tok, batched=True)\n",
    "\n",
    "ht_train_ds.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label_local\"])\n",
    "ht_val_ds.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label_local\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5a0ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: shared encoder + per-group head, with per-example loss\n",
    "class HierTransformer(nn.Module):\n",
    "    def __init__(self, base_model_name: str, group_num_labels: dict[str,int], class_weights: dict[str, torch.Tensor] | None = None):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(base_model_name)\n",
    "        hidden = self.encoder.config.hidden_size\n",
    "        self.group_num_labels = group_num_labels\n",
    "        self.heads = nn.ModuleDict({g: nn.Linear(hidden, n) for g, n in group_num_labels.items()})\n",
    "        self.class_weights = class_weights or {}\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, groups, labels=None):\n",
    "        enc = self.encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0]  # CLS\n",
    "        # logits per example computed with its group head\n",
    "        logits_list = []\n",
    "        loss = None\n",
    "\n",
    "        if labels is not None:\n",
    "            losses = []\n",
    "\n",
    "        for i in range(enc.size(0)):\n",
    "            g = groups[i]\n",
    "            head = self.heads[g]\n",
    "            logits_i = head(enc[i])  # [n_labels_in_group]\n",
    "            logits_list.append(logits_i)\n",
    "\n",
    "            if labels is not None:\n",
    "                w = self.class_weights.get(g, None)\n",
    "                loss_fn = nn.CrossEntropyLoss(weight=w.to(logits_i.device) if w is not None else None)\n",
    "                losses.append(loss_fn(logits_i.unsqueeze(0), labels[i].unsqueeze(0)))\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = torch.stack(losses).mean()\n",
    "\n",
    "        return {\"loss\": loss, \"logits_list\": logits_list}\n",
    "\n",
    "\n",
    "# Optional per-group class weights (inverse frequency)\n",
    "class_w = None\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    class_w = {}\n",
    "    for g, sub in ht_train_ex.groupby(\"group\"):\n",
    "        counts = sub[\"label_local\"].value_counts().sort_index()\n",
    "        w = (counts.sum() / (counts + 1e-9)).values.astype(np.float32)\n",
    "        w = w / w.mean()\n",
    "        class_w[str(g)] = torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "ht_model = HierTransformer(HT_MODEL, group_num_labels, class_weights=class_w).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e03b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom trainer: we need to pass groups and compute loss\n",
    "class HTTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"label_local\")\n",
    "        # groups come from an external list since HF Dataset doesn't keep strings as tensors nicely\n",
    "        # We'll attach them via a side-channel using the batch indices.\n",
    "        raise RuntimeError(\"This cell is a placeholder; see next cell for the working dataloader approach.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4286f671",
   "metadata": {},
   "source": [
    "## Important note (implementation detail)\n",
    "\n",
    "HF `Trainer` doesn't natively handle per-example *string* groups cleanly.\n",
    "\n",
    "To keep the notebook **working and production-realistic**, we train the hierarchical transformer using a small custom\n",
    "training loop (still simple), and then evaluate Top‑K.\n",
    "\n",
    "This avoids fragile Trainer hacks and is easier to port into Azure ML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8fe57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Build torch datasets with groups stored alongside tensors\n",
    "def build_torch_dataset(ds: Dataset, groups: list[str]):\n",
    "    # ds already has tensors for input_ids/attention_mask/label_local\n",
    "    assert len(ds) == len(groups)\n",
    "    return list(zip(ds[\"input_ids\"], ds[\"attention_mask\"], ds[\"label_local\"], groups))\n",
    "\n",
    "train_groups_list = ht_train_ex[\"group\"].astype(str).tolist()\n",
    "val_groups_list   = ht_val_ex[\"group\"].astype(str).tolist()\n",
    "\n",
    "train_torch = build_torch_dataset(ht_train_ds, train_groups_list)\n",
    "val_torch   = build_torch_dataset(ht_val_ds, val_groups_list)\n",
    "\n",
    "def collate(batch):\n",
    "    input_ids = torch.stack([b[0] for b in batch])\n",
    "    attn = torch.stack([b[1] for b in batch])\n",
    "    labels = torch.stack([b[2] for b in batch])\n",
    "    groups = [b[3] for b in batch]\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attn, \"labels\": labels, \"groups\": groups}\n",
    "\n",
    "train_loader = DataLoader(train_torch, batch_size=HT_BS, shuffle=True, collate_fn=collate)\n",
    "val_loader   = DataLoader(val_torch, batch_size=HT_BS, shuffle=False, collate_fn=collate)\n",
    "\n",
    "# Optimizer\n",
    "optim = torch.optim.AdamW(ht_model.parameters(), lr=HT_LR, weight_decay=HT_WEIGHT_DECAY)\n",
    "\n",
    "def train_one_epoch():\n",
    "    ht_model.train()\n",
    "    total=0.0; n=0\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        out = ht_model(\n",
    "            input_ids=batch[\"input_ids\"].to(device),\n",
    "            attention_mask=batch[\"attention_mask\"].to(device),\n",
    "            groups=batch[\"groups\"],\n",
    "            labels=batch[\"labels\"].to(device),\n",
    "        )\n",
    "        loss = out[\"loss\"]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total += float(loss.detach().cpu())\n",
    "        n += 1\n",
    "    return total / max(1,n)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_loss():\n",
    "    ht_model.eval()\n",
    "    total=0.0; n=0\n",
    "    for batch in val_loader:\n",
    "        out = ht_model(\n",
    "            input_ids=batch[\"input_ids\"].to(device),\n",
    "            attention_mask=batch[\"attention_mask\"].to(device),\n",
    "            groups=batch[\"groups\"],\n",
    "            labels=batch[\"labels\"].to(device),\n",
    "        )\n",
    "        total += float(out[\"loss\"].detach().cpu())\n",
    "        n += 1\n",
    "    return total / max(1,n)\n",
    "\n",
    "t0 = time.time()\n",
    "best = 1e9\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, HT_EPOCHS+1):\n",
    "    tr = train_one_epoch()\n",
    "    va = eval_loss()\n",
    "    print(f\"Epoch {epoch}: train_loss={tr:.4f} val_loss={va:.4f}\")\n",
    "    if va < best:\n",
    "        best = va\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in ht_model.state_dict().items()}\n",
    "\n",
    "if best_state is not None:\n",
    "    ht_model.load_state_dict(best_state)\n",
    "\n",
    "print(\"Hierarchical transformer trained in %.1fs\" % (time.time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec4950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate hierarchical transformer demand_id Top-1 / Top-K on validation using predicted group (same as CE fairness)\n",
    "\n",
    "@torch.no_grad()\n",
    "def ht_predict_topk(text: str, group: str, k: int) -> list[str]:\n",
    "    group = str(group)\n",
    "    if group not in group_label_list:\n",
    "        return []\n",
    "    labs = group_label_list[group]\n",
    "    # tokenize\n",
    "    batch = ht_tokenizer(text, truncation=True, padding=\"max_length\", max_length=HT_MAX_LEN, return_tensors=\"pt\")\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    enc = ht_model.encoder(**batch).last_hidden_state[:, 0]\n",
    "    logits = ht_model.heads[group](enc[0])  # [n_labels]\n",
    "    probs = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    order = np.argsort(-probs)[:k]\n",
    "    return [labs[i] for i in order]\n",
    "\n",
    "# group prediction for the SAME val rows we evaluate (val_seen_df aligns with ht_val_ex)\n",
    "group_probs_seen = group_cal.predict_proba(val_seen_df[TEXT_COL].astype(str))\n",
    "group_classes = group_cal.classes_\n",
    "seen_top1_group = group_classes[np.argmax(group_probs_seen, axis=1)]\n",
    "\n",
    "y_true_seen = val_seen_df[DEMAND_COL].astype(str).tolist()\n",
    "texts_seen = val_seen_df[TEXT_COL].astype(str).tolist()\n",
    "\n",
    "ht_top1=0; ht_topk=0; valid=0\n",
    "for text, true_lab, g in zip(texts_seen, y_true_seen, seen_top1_group):\n",
    "    preds = ht_predict_topk(text, str(g), TOP_K)\n",
    "    if not preds:\n",
    "        continue\n",
    "    valid += 1\n",
    "    ht_top1 += int(preds[0] == true_lab)\n",
    "    ht_topk += int(true_lab in preds)\n",
    "\n",
    "ht_top1_acc = ht_top1 / max(1,valid)\n",
    "ht_topk_acc = ht_topk / max(1,valid)\n",
    "\n",
    "print(\"HT evaluated rows:\", valid, \"/\", len(val_seen_df))\n",
    "print(\"HT Top-1:\", ht_top1_acc)\n",
    "print(f\"HT Top-{TOP_K}:\", ht_topk_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa8e5eb",
   "metadata": {},
   "source": [
    "## 5) Summary comparison table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f09862",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame([\n",
    "    {\"model\": \"Cross-Encoder (desc-aware) + group\", \"top1\": ce_top1_acc, f\"top{TOP_K}\": ce_topk_acc, \"eval_rows\": valid},\n",
    "    {\"model\": \"Hierarchical Transformer (group heads) + group\", \"top1\": ht_top1_acc, f\"top{TOP_K}\": ht_topk_acc, \"eval_rows\": valid},\n",
    "])\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c523be",
   "metadata": {},
   "source": [
    "## Notes on fairness\n",
    "\n",
    "- Both models use the **same Stage A group predictor** and the **same val split**.\n",
    "- For the hierarchical transformer we also require the val label to exist in train within its group (`val_seen_df`),\n",
    "  otherwise it is literally un-predictable. (Cross-encoder can still score unseen labels if it has descriptions,\n",
    "  but we keep evaluation conservative by using the same seen subset for the HT metric.)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
