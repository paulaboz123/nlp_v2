{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53a46ce4",
   "metadata": {},
   "source": [
    "# Demand label classification (200+ labels) — best practical solution\n",
    "\n",
    "Best pattern for your case:\n",
    "- many (200+) similar, hierarchical labels\n",
    "- long-tail imbalance\n",
    "- label `description` exists\n",
    "- English text\n",
    "\n",
    "✅ Pipeline:\n",
    "1) (separate) Relevance filter -> keep `relevant==1`\n",
    "2) Group classifier (TF-IDF + LinearSVC) to predict coarse group\n",
    "3) Description-aware **cross-encoder** to score labels within the predicted group\n",
    "\n",
    "This notebook implements steps (2) and (3), and reports Top-1 / Top-5 accuracy on validation.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs\n",
    "- `dataset.csv` with columns: `text`, `demand_id`, `relevant`/`relevance`, and `group_id` (or mergeable from labels file)\n",
    "- `labels.csv` with columns: `demand_id`, `description` (and optionally `group_id`)\n",
    "\n",
    "---\n",
    "\n",
    "## Notes\n",
    "- Cross-encoder training is heavier; GPU recommended. It can still run on CPU for a smoke-test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b375d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CONFIG (EDIT THESE)\n",
    "# =========================\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_CSV = Path(\"dataset.csv\")\n",
    "LABELS_CSV  = Path(\"labels.csv\")\n",
    "\n",
    "TEXT_COL = \"text\"\n",
    "DEMAND_COL = \"demand_id\"\n",
    "GROUP_COL = \"group_id\"\n",
    "REL_COL_CANDIDATES = [\"relevant\", \"relevance\"]\n",
    "\n",
    "SEED = 42\n",
    "TEST_SIZE = 0.30\n",
    "\n",
    "# Cross-encoder backbone (English)\n",
    "CROSS_ENCODER_MODEL = \"microsoft/deberta-v3-base\"\n",
    "MAX_LEN = 256\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 8\n",
    "LR = 2e-5\n",
    "\n",
    "NEG_PER_POS = 3\n",
    "TOP_K = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c6dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "RNG = np.random.default_rng(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed06d2be",
   "metadata": {},
   "source": [
    "## 1) Load data + detect relevance column + merge descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3effc83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET_CSV)\n",
    "labels = pd.read_csv(LABELS_CSV)\n",
    "\n",
    "# detect relevance column\n",
    "rel_col = None\n",
    "for c in REL_COL_CANDIDATES:\n",
    "    if c in df.columns:\n",
    "        rel_col = c\n",
    "        break\n",
    "if rel_col is None:\n",
    "    raise ValueError(f\"Missing relevance column. Tried: {REL_COL_CANDIDATES}. Have: {list(df.columns)}\")\n",
    "\n",
    "# required columns\n",
    "for c in [TEXT_COL, DEMAND_COL]:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f\"Dataset missing required column: {c}\")\n",
    "\n",
    "for c in [DEMAND_COL, \"description\"]:\n",
    "    if c not in labels.columns:\n",
    "        raise ValueError(f\"Labels file missing required column: {c}\")\n",
    "\n",
    "df[rel_col] = df[rel_col].fillna(0).astype(int)\n",
    "df[DEMAND_COL] = df[DEMAND_COL].astype(str)\n",
    "labels[DEMAND_COL] = labels[DEMAND_COL].astype(str)\n",
    "\n",
    "# Add group_id if needed (from labels)\n",
    "if GROUP_COL not in df.columns and GROUP_COL in labels.columns:\n",
    "    df = df.merge(labels[[DEMAND_COL, GROUP_COL]], on=DEMAND_COL, how=\"left\")\n",
    "\n",
    "if GROUP_COL not in df.columns:\n",
    "    raise ValueError(f\"Missing {GROUP_COL} in dataset. Provide it in dataset.csv or labels.csv (and set GROUP_COL).\")\n",
    "\n",
    "# Merge descriptions into dataset\n",
    "df = df.merge(labels[[DEMAND_COL, \"description\"]], on=DEMAND_COL, how=\"left\")\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Relevant distribution:\", df[rel_col].value_counts().to_dict())\n",
    "print(\"Unique demand labels:\", df[DEMAND_COL].nunique())\n",
    "print(\"Unique groups:\", df[GROUP_COL].nunique())\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df21e5d2",
   "metadata": {},
   "source": [
    "## 2) Keep only relevant==1 for label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e27cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel = df[df[rel_col] == 1].copy()\n",
    "df_rel = df_rel.dropna(subset=[TEXT_COL, DEMAND_COL, GROUP_COL, \"description\"]).copy()\n",
    "\n",
    "print(\"Relevant-only rows:\", len(df_rel))\n",
    "print(\"Unique demand labels (relevant):\", df_rel[DEMAND_COL].nunique())\n",
    "print(\"Unique groups (relevant):\", df_rel[GROUP_COL].nunique())\n",
    "df_rel.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033ff1d2",
   "metadata": {},
   "source": [
    "## 3) Train/val split (stratify by group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6822844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    df_rel,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=SEED,\n",
    "    stratify=df_rel[GROUP_COL] if df_rel[GROUP_COL].nunique() > 1 else None,\n",
    ")\n",
    "\n",
    "print(\"Train:\", train_df.shape, \"Val:\", val_df.shape)\n",
    "print(\"Train groups:\", train_df[GROUP_COL].nunique(), \"Val groups:\", val_df[GROUP_COL].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3d8442",
   "metadata": {},
   "source": [
    "## 4) Stage A — Group classifier (TF-IDF + LinearSVC) + calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d9f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(min_df=2, max_df=0.95, ngram_range=(1,3))),\n",
    "    (\"clf\", LinearSVC(class_weight=\"balanced\", max_iter=8000)),\n",
    "])\n",
    "\n",
    "group_pipe.fit(train_df[TEXT_COL].astype(str), train_df[GROUP_COL].astype(str))\n",
    "\n",
    "group_cal = CalibratedClassifierCV(group_pipe, method=\"sigmoid\", cv=3)\n",
    "group_cal.fit(train_df[TEXT_COL].astype(str), train_df[GROUP_COL].astype(str))\n",
    "\n",
    "pred_groups = group_cal.predict(val_df[TEXT_COL].astype(str))\n",
    "print(classification_report(val_df[GROUP_COL].astype(str), pred_groups, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c033634b",
   "metadata": {},
   "source": [
    "## 5) Build group -> candidate labels (demand_id, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a5d6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_rel = labels.merge(df_rel[[DEMAND_COL, GROUP_COL]].drop_duplicates(), on=DEMAND_COL, how=\"inner\")\n",
    "labels_rel = labels_rel.dropna(subset=[GROUP_COL, \"description\"])\n",
    "\n",
    "group_to_labels = {}\n",
    "for g, sub in labels_rel.groupby(GROUP_COL):\n",
    "    pairs = list(zip(sub[DEMAND_COL].astype(str).tolist(), sub[\"description\"].astype(str).tolist()))\n",
    "    seen=set()\n",
    "    uniq=[]\n",
    "    for did, desc in pairs:\n",
    "        if did in seen:\n",
    "            continue\n",
    "        seen.add(did)\n",
    "        uniq.append((did, desc))\n",
    "    group_to_labels[str(g)] = uniq\n",
    "\n",
    "print(\"Example group sizes (first 10):\")\n",
    "for g, pairs in list(group_to_labels.items())[:10]:\n",
    "    print(g, \"labels:\", len(pairs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebda95b",
   "metadata": {},
   "source": [
    "## 6) Build cross-encoder training pairs (positive + negatives within same group)\n",
    "\n",
    "- Positive: (text, correct description) -> 1\n",
    "- Negatives: (text, other descriptions in same group) -> 0\n",
    "\n",
    "This targets the \"very similar labels\" problem directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef445a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairs(df_part: pd.DataFrame, neg_per_pos: int) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for _, r in df_part.iterrows():\n",
    "        text = str(r[TEXT_COL])\n",
    "        demand = str(r[DEMAND_COL])\n",
    "        group = str(r[GROUP_COL])\n",
    "        desc_pos = str(r[\"description\"])\n",
    "\n",
    "        rows.append({\"text\": text, \"description\": desc_pos, \"labels\": 1, \"group_id\": group, \"true_demand\": demand})\n",
    "\n",
    "        candidates = group_to_labels.get(group, [])\n",
    "        neg_pool = [(d, desc) for d, desc in candidates if d != demand]\n",
    "        if not neg_pool:\n",
    "            continue\n",
    "        take = min(neg_per_pos, len(neg_pool))\n",
    "        neg_idx = RNG.choice(len(neg_pool), size=take, replace=False)\n",
    "        for i in np.atleast_1d(neg_idx):\n",
    "            _, desc_neg = neg_pool[int(i)]\n",
    "            rows.append({\"text\": text, \"description\": str(desc_neg), \"labels\": 0, \"group_id\": group, \"true_demand\": demand})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "pair_train = make_pairs(train_df, NEG_PER_POS)\n",
    "pair_val = make_pairs(val_df, NEG_PER_POS)\n",
    "\n",
    "print(\"Pair-train:\", pair_train.shape, \"pos rate:\", pair_train[\"labels\"].mean())\n",
    "print(\"Pair-val  :\", pair_val.shape, \"pos rate:\", pair_val[\"labels\"].mean())\n",
    "pair_train.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf48c2",
   "metadata": {},
   "source": [
    "## 7) Train cross-encoder (transformers Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be22e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CROSS_ENCODER_MODEL)\n",
    "\n",
    "def tok_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        batch[\"description\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "ds_train = Dataset.from_pandas(pair_train[[\"text\",\"description\",\"labels\"]], preserve_index=False).map(tok_fn, batched=True)\n",
    "ds_val   = Dataset.from_pandas(pair_val[[\"text\",\"description\",\"labels\"]], preserve_index=False).map(tok_fn, batched=True)\n",
    "\n",
    "ds_train.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n",
    "ds_val.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(CROSS_ENCODER_MODEL, num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ce_out\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_val,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac88adb2",
   "metadata": {},
   "source": [
    "## 8) Evaluate demand_id prediction (Top-1 / Top-K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36babe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def score_pairs(text: str, descriptions: list[str]) -> np.ndarray:\n",
    "    batch = tokenizer([text]*len(descriptions), descriptions, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    batch = {k: v.to(trainer.model.device) for k, v in batch.items()}\n",
    "    logits = trainer.model(**batch).logits\n",
    "    probs = F.softmax(logits, dim=-1)[:, 1].detach().cpu().numpy()\n",
    "    return probs\n",
    "\n",
    "def predict_topk_for_row(text: str, group: str, k: int = 5):\n",
    "    pairs = group_to_labels.get(str(group), [])\n",
    "    if not pairs:\n",
    "        return []\n",
    "    demand_ids = [d for d, _ in pairs]\n",
    "    descs = [desc for _, desc in pairs]\n",
    "    scores = score_pairs(text, descs)\n",
    "    order = np.argsort(-scores)\n",
    "    return [demand_ids[i] for i in order[:k]]\n",
    "\n",
    "# group prediction top-1\n",
    "group_probs = group_cal.predict_proba(val_df[TEXT_COL].astype(str))\n",
    "group_classes = group_cal.classes_\n",
    "top1_groups = group_classes[np.argmax(group_probs, axis=1)]\n",
    "\n",
    "y_true = val_df[DEMAND_COL].astype(str).tolist()\n",
    "texts = val_df[TEXT_COL].astype(str).tolist()\n",
    "\n",
    "top1 = 0\n",
    "topk = 0\n",
    "valid = 0\n",
    "\n",
    "for t, true_lab, g in zip(texts, y_true, top1_groups):\n",
    "    preds = predict_topk_for_row(t, str(g), k=TOP_K)\n",
    "    if not preds:\n",
    "        continue\n",
    "    valid += 1\n",
    "    if preds[0] == true_lab:\n",
    "        top1 += 1\n",
    "    if true_lab in preds:\n",
    "        topk += 1\n",
    "\n",
    "print(\"Evaluated rows (had group candidates):\", valid, \"out of\", len(val_df))\n",
    "print(f\"Top-1 accuracy: {top1/max(1,valid):.4f}\")\n",
    "print(f\"Top-{TOP_K} accuracy: {topk/max(1,valid):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587d95a2",
   "metadata": {},
   "source": [
    "## 9) Save artifacts for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce00a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, joblib\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR = Path(\"demand_classifier_artifacts\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "joblib.dump(group_cal, OUT_DIR / \"group_model_calibrated.joblib\")\n",
    "\n",
    "trainer.model.save_pretrained(OUT_DIR / \"cross_encoder\")\n",
    "tokenizer.save_pretrained(OUT_DIR / \"cross_encoder\")\n",
    "\n",
    "mapping = {g: [{\"demand_id\": did, \"description\": desc} for did, desc in pairs] for g, pairs in group_to_labels.items()}\n",
    "(OUT_DIR / \"group_to_labels.json\").write_text(json.dumps(mapping, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Saved to:\", OUT_DIR.resolve())\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
